{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable_glorot(output_dim):\n",
    "\tinput_dim = output_dim\n",
    "\tinit_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "\tinitial = np.random.uniform(-init_range, init_range,\n",
    "\t\t\t\t\t\t\t\t(input_dim, output_dim))\n",
    "\n",
    "\treturn initial\n",
    "\n",
    "def pad_HR_adj(label, split):\n",
    "\tlabel = F.pad(label, (split, split, split, split), \"constant\", 0)\n",
    "\tlabel.fill_diagonal_(1)\n",
    "\treturn label\n",
    "\n",
    "def normalize_adj_torch(mx):\n",
    "\trowsum = mx.sum(1)\n",
    "\tr_inv_sqrt = torch.pow(rowsum, -0.5).flatten()\n",
    "\tr_inv_sqrt[torch.isinf(r_inv_sqrt)] = 0.\n",
    "\tr_mat_inv_sqrt = torch.diag(r_inv_sqrt)\n",
    "\tmx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "\tmx = torch.transpose(mx, 0, 1)\n",
    "\tmx = torch.matmul(mx, r_mat_inv_sqrt)\n",
    "\treturn mx\n",
    "\n",
    "def unpad(data, split):\n",
    "\tidx_0 = data.shape[0]-split\n",
    "\tidx_1 = data.shape[1]-split\n",
    "\ttrain = data[split:idx_0, split:idx_1]\n",
    "\treturn train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphUnpool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GraphUnpool, self).__init__()\n",
    "\n",
    "    def forward(self, A, X, idx):\n",
    "        new_X = torch.zeros([A.shape[0], X.shape[1]]).to(device)\n",
    "        new_X[idx] = X\n",
    "        return A, new_X\n",
    "\n",
    "class GraphPool(nn.Module):\n",
    "    def __init__(self, k, in_dim):\n",
    "        super(GraphPool, self).__init__()\n",
    "        self.k = k\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        scores = self.proj(X)\n",
    "        # scores = torch.abs(scores)\n",
    "        scores = torch.squeeze(scores)\n",
    "        scores = self.sigmoid(scores/100)\n",
    "        num_nodes = A.shape[0]\n",
    "        values, idx = torch.topk(scores, int(self.k*num_nodes))\n",
    "        new_X = X[idx, :]\n",
    "        values = torch.unsqueeze(values, -1)\n",
    "        new_X = torch.mul(new_X, values)\n",
    "        A = A[idx, :]\n",
    "        A = A[:, idx]\n",
    "        return A, new_X, idx\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "        self.drop = nn.Dropout(p=0)\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        X = self.drop(X)\n",
    "        X = torch.matmul(A, X)\n",
    "        X = self.proj(X)\n",
    "        return X\n",
    "\n",
    "class GraphUnet(nn.Module):\n",
    "    def __init__(self, ks, in_dim, out_dim, dim=320):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.ks = ks\n",
    "\n",
    "        self.start_gcn = GCN(in_dim, dim)\n",
    "        self.bottom_gcn = GCN(dim, dim)\n",
    "        self.end_gcn = GCN(2 * dim, out_dim)\n",
    "        self.down_gcns = nn.ModuleList()\n",
    "        self.up_gcns = nn.ModuleList()\n",
    "        self.pools = nn.ModuleList()\n",
    "        self.unpools = nn.ModuleList()\n",
    "        self.l_n = len(ks)\n",
    "        for i in range(self.l_n):\n",
    "            self.down_gcns.append(GCN(dim, dim))\n",
    "            self.up_gcns.append(GCN(dim, dim))\n",
    "            self.pools.append(GraphPool(ks[i], dim))\n",
    "            self.unpools.append(GraphUnpool())\n",
    "\n",
    "    def forward(self, A, X):\n",
    "        adj_ms = []\n",
    "        indices_list = []\n",
    "        down_outs = []\n",
    "        X = self.start_gcn(A, X)\n",
    "        start_gcn_outs = X\n",
    "        org_X = X\n",
    "        for i in range(self.l_n):\n",
    "\n",
    "            X = self.down_gcns[i](A, X)\n",
    "            adj_ms.append(A)\n",
    "            down_outs.append(X)\n",
    "            A, X, idx = self.pools[i](A, X)\n",
    "            indices_list.append(idx)\n",
    "        X = self.bottom_gcn(A, X)\n",
    "        for i in range(self.l_n):\n",
    "            up_idx = self.l_n - i - 1\n",
    "\n",
    "            A, idx = adj_ms[up_idx], indices_list[up_idx]\n",
    "            A, X = self.unpools[i](A, X, idx)\n",
    "            X = self.up_gcns[i](A, X)\n",
    "            X = X.add(down_outs[up_idx])\n",
    "        X = torch.cat([X, org_X], 1)\n",
    "        X = self.end_gcn(A, X)\n",
    "\n",
    "        return X, start_gcn_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSRLayer(nn.Module):\n",
    "\n",
    "\tdef __init__(self, hr_dim, device):\n",
    "\t\tsuper(GSRLayer, self).__init__()\n",
    "\n",
    "\t\tself.device = device\n",
    "\t\tself.weights = torch.from_numpy(\n",
    "\t\t\tweight_variable_glorot(hr_dim)).to(device=self.device, dtype=torch.float32)\n",
    "\t\tself.weights = torch.nn.Parameter(\n",
    "\t\t\tdata=self.weights, requires_grad=True)\n",
    "\n",
    "\tdef forward(self, A, X):\n",
    "\t\twith torch.autograd.set_detect_anomaly(True):\n",
    "\n",
    "\t\t\tlr = A\n",
    "\t\t\tlr_dim = lr.shape[0]\n",
    "\t\t\tf = X\n",
    "\t\t\teig_val_lr, U_lr = torch.linalg.eigh(lr, UPLO='U')\n",
    "\n",
    "\t\t\t# U_lr = torch.abs(U_lr)\n",
    "\t\t\teye_mat = torch.eye(lr_dim).to(device=self.device, dtype=torch.float32)\n",
    "\t\t\ts_d = torch.cat((eye_mat, eye_mat), 0)\n",
    "\n",
    "\t\t\ta = torch.matmul(self.weights, s_d)\n",
    "\t\t\tb = torch.matmul(a, torch.t(U_lr))\n",
    "\t\t\tf_d = torch.matmul(b, f)\n",
    "\t\t\tf_d = torch.abs(f_d)\n",
    "\t\t\tf_d = f_d.fill_diagonal_(1)\n",
    "\t\t\tadj = f_d\n",
    "\n",
    "\t\t\tX = torch.mm(adj, adj.t())\n",
    "\t\t\tX = (X + X.t())/2\n",
    "\t\t\tX = X.fill_diagonal_(1)\n",
    "\t\treturn adj, torch.abs(X)\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "\t\"\"\"\n",
    "\tSimple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, in_features, out_features, dropout, act=F.relu):\n",
    "\t\tsuper(GraphConvolution, self).__init__()\n",
    "\t\tself.in_features = in_features\n",
    "\t\tself.out_features = out_features\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.act = act\n",
    "\t\tself.weight = torch.nn.Parameter(\n",
    "\t\t\ttorch.FloatTensor(in_features, out_features))\n",
    "\t\tself.reset_parameters()\n",
    "\n",
    "\tdef reset_parameters(self):\n",
    "\t\ttorch.nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "\tdef forward(self, input, adj):\n",
    "\t\tinput = F.dropout(input, self.dropout, self.training)\n",
    "\t\tsupport = torch.mm(input, self.weight)\n",
    "\t\toutput = torch.mm(adj, support)\n",
    "\t\toutput = self.act(output)\n",
    "\t\treturn output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AGSRNet(nn.Module):\n",
    "\n",
    "    def __init__(self, ks, args, device):\n",
    "        super(AGSRNet, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.lr_dim = args[\"lr_dim\"]\n",
    "        self.hr_dim = args[\"hr_dim\"]\n",
    "        self.hidden_dim = args[\"hidden_dim\"]\n",
    "        self.layer = GSRLayer(self.hr_dim, device)\n",
    "        self.net = GraphUnet(ks, self.lr_dim, self.hr_dim)\n",
    "        self.gc1 = GraphConvolution(\n",
    "            self.hr_dim, self.hidden_dim, 0, act=F.relu)\n",
    "        self.gc2 = GraphConvolution(\n",
    "            self.hidden_dim, self.hr_dim, 0, act=F.relu)\n",
    "\n",
    "    def forward(self, lr):\n",
    "        with torch.autograd.set_detect_anomaly(True):\n",
    "            I = torch.eye(self.lr_dim).to(device=self.device, dtype=torch.float32)\n",
    "            A = normalize_adj_torch(lr).to(device=self.device, dtype=torch.float32)\n",
    "\n",
    "            self.net_outs, self.start_gcn_outs = self.net(A, I)\n",
    "            self.outputs, self.Z = self.layer(A, self.net_outs)\n",
    "\n",
    "            self.hidden1 = self.gc1(self.Z, self.outputs)\n",
    "            self.hidden2 = self.gc2(self.hidden1, self.outputs)\n",
    "            z = self.hidden2\n",
    "\n",
    "            z = (z + z.t())/2\n",
    "            z = z.fill_diagonal_(1)\n",
    "\n",
    "        return torch.abs(z), self.net_outs, self.start_gcn_outs, self.outputs\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "\tdef __init__(self, n1, n2, args):\n",
    "\t\tsuper(Dense, self).__init__()\n",
    "\t\tself.weights = torch.nn.Parameter(\n",
    "\t\t\ttorch.FloatTensor(n1, n2).to(device), requires_grad=True)\n",
    "\t\tnn.init.normal_(self.weights, mean=args[\"mean_dense\"], std=args[\"std_dense\"])\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tnp.random.seed(1)\n",
    "\t\ttorch.manual_seed(1)\n",
    "\n",
    "\t\tout = torch.mm(x, self.weights)\n",
    "\t\treturn out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.dense_1 = Dense(args[\"hr_dim\"], args[\"hr_dim\"], args)\n",
    "        self.relu_1 = nn.ReLU(inplace=False)\n",
    "        self.dense_2 = Dense(args[\"hr_dim\"], args[\"hr_dim\"], args)\n",
    "        self.relu_2 = nn.ReLU(inplace=False)\n",
    "        self.dense_3 = Dense(args[\"hr_dim\"], 1, args)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        np.random.seed(1)\n",
    "        torch.manual_seed(1)\n",
    "        dc_den1 = self.relu_1(self.dense_1(inputs))\n",
    "        dc_den2 = self.relu_2(self.dense_2(dc_den1))\n",
    "        output = dc_den2\n",
    "        output = self.dense_3(dc_den2)\n",
    "        output = self.sigmoid(output)\n",
    "        return torch.abs(output)\n",
    "\n",
    "\n",
    "def gaussian_noise_layer(input_layer, args):\n",
    "    z = torch.empty_like(input_layer)\n",
    "    noise = z.normal_(mean=args[\"mean_gaussian\"], std=args[\"std_gaussian\"])\n",
    "    z = torch.abs(input_layer + noise)\n",
    "\n",
    "    z = (z + z.t())/2\n",
    "    z = z.fill_diagonal_(1)\n",
    "    return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train(model, subjects_adj, subjects_labels, args):\n",
    "\n",
    "\tbce_loss = nn.BCELoss()\n",
    "\tnetD = Discriminator(args)\n",
    "\tprint(netD)\n",
    "\toptimizerG = optim.Adam(model.parameters(), lr=args[\"lr\"])\n",
    "\toptimizerD = optim.Adam(netD.parameters(), lr=args[\"lr\"])\n",
    "\n",
    "\tall_epochs_loss = []\n",
    "\tfor epoch in range(args[\"epochs\"]):\n",
    "\t\tprint(\"Epoch: \", epoch)\n",
    "\t\twith torch.autograd.set_detect_anomaly(True):\n",
    "\t\t\tepoch_loss = []\n",
    "\t\t\tepoch_error = []\n",
    "\t\t\tfor lr, hr in zip(subjects_adj, subjects_labels):\n",
    "\t\t\t\toptimizerD.zero_grad()\n",
    "\t\t\t\toptimizerG.zero_grad()\n",
    "\n",
    "\t\t\t\thr = pad_HR_adj(hr, args[\"padding\"])\n",
    "\t\t\t\tlr = lr.to(device=device, dtype=torch.float32)\n",
    "\t\t\t\tpadded_hr = hr.to(device=device, dtype=torch.float32)\n",
    "\n",
    "\t\t\t\teig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "\t\t\t\tU_hr = U_hr.to(device)\n",
    "\n",
    "\t\t\t\tmodel_outputs, net_outs, start_gcn_outs, layer_outs = model(lr)\n",
    "\n",
    "\t\t\t\tmse_loss = args[\"lmbda\"] * criterion(net_outs, start_gcn_outs) + criterion(\n",
    "\t\t\t\t\tmodel.layer.weights, U_hr) + criterion(model_outputs, padded_hr)\n",
    "\n",
    "\t\t\t\terror = criterion(model_outputs, padded_hr)\n",
    "\t\t\t\treal_data = model_outputs.detach()\n",
    "\t\t\t\tfake_data = gaussian_noise_layer(padded_hr, args)\n",
    "\n",
    "\t\t\t\td_real = netD(real_data)\n",
    "\t\t\t\td_fake = netD(fake_data)\n",
    "\n",
    "\t\t\t\tdc_loss_real = bce_loss(d_real, torch.ones(args[\"hr_dim\"], 1).to(device))\n",
    "\t\t\t\tdc_loss_fake = bce_loss(d_fake, torch.zeros(args[\"hr_dim\"], 1).to(device))\n",
    "\t\t\t\tdc_loss = dc_loss_real + dc_loss_fake\n",
    "\n",
    "\t\t\t\tdc_loss.backward()\n",
    "\t\t\t\toptimizerD.step()\n",
    "\n",
    "\t\t\t\td_fake = netD(gaussian_noise_layer(padded_hr, args))\n",
    "\n",
    "\t\t\t\tgen_loss = bce_loss(d_fake, torch.ones(args[\"hr_dim\"], 1).to(device))\n",
    "\t\t\t\tgenerator_loss = gen_loss + mse_loss\n",
    "\t\t\t\tgenerator_loss.backward()\n",
    "\t\t\t\toptimizerG.step()\n",
    "\n",
    "\t\t\t\tepoch_loss.append(generator_loss.item())\n",
    "\t\t\t\tepoch_error.append(error.item())\n",
    "\n",
    "\t\t\tprint(\"Epoch: \", epoch, \"Loss: \", np.mean(epoch_loss),\n",
    "\t\t\t\t\"Error: \", np.mean(epoch_error)*100, \"%\")\n",
    "\t\t\tall_epochs_loss.append(np.mean(epoch_loss))\n",
    "\n",
    "def test(model, test_adj, test_labels, args):\n",
    "\n",
    "\tg_t = []\n",
    "\ttest_error = []\n",
    "\tpreds_list = []\n",
    "\n",
    "\t# i = 0\n",
    "\n",
    "\tfor lr, hr in zip(test_adj, test_labels):\n",
    "\t\tall_zeros_lr = not np.any(lr)\n",
    "\t\tall_zeros_hr = not np.any(hr)\n",
    "\t\tif all_zeros_lr == False and all_zeros_hr == False:\n",
    "\t\t\tlr = lr.type(torch.FloatTensor)\n",
    "\t\t\thr.fill_diagonal_(1)\n",
    "\t\t\thr = pad_HR_adj(hr, args[\"padding\"])\n",
    "\t\t\thr = hr.type(torch.FloatTensor)\n",
    "\t\t\tpreds, a, b, c = model(lr)\n",
    "\n",
    "\t\t\tpreds_list.append(preds.flatten().detach().numpy())\n",
    "\t\t\terror = criterion(preds, hr)\n",
    "\t\t\tg_t.append(hr.flatten())\n",
    "\t\t\tprint(error.item())\n",
    "\t\t\ttest_error.append(error.item())\n",
    "\t\t\t# i += 1\n",
    "\n",
    "\tprint(\"Test error MSE: \", np.mean(test_error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"epochs\": 5,\n",
    "\t\"lr\": 0.0001,\n",
    "\t\"lmbda\": 0.01,\n",
    "\t\"lr_dim\": 160,\n",
    "\t\"hr_dim\": 320,\n",
    "\t\"hidden_dim\": 320,\n",
    "    \"padding\": 26,\n",
    "\t\"mean_dense\": 0,\n",
    "\t\"std_dense\": 0.01,\n",
    "\t\"mean_gaussian\": 0,\n",
    "\t\"std_gaussian\": 0.01\n",
    "}\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "\n",
    "model = AGSRNet(ks, args, device).to(device)\n",
    "discriminator = Discriminator(args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \"data/lr_train.pt\"\n",
    "lr_train = torch.load(\"data/lr_train.pt\")\n",
    "hr_train = torch.load(\"data/hr_train.pt\")\n",
    "lr_test = torch.load(\"data/lr_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.matrix_vectorizer import MatrixVectorizer\n",
    "mv = MatrixVectorizer()\n",
    "\n",
    "lr_train = torch.stack([mv.anti_vectorize(lr_train[i], 160) for i in range(len(lr_train))]).to(device)\n",
    "hr_train = torch.stack([mv.anti_vectorize(hr_train[i], 268) for i in range(len(hr_train))]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (dense_1): Dense()\n",
      "  (relu_1): ReLU()\n",
      "  (dense_2): Dense()\n",
      "  (relu_2): ReLU()\n",
      "  (dense_3): Dense()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Epoch:  0\n",
      "Epoch:  0 Loss:  1.3868542049459354 Error:  5.014030966617747 %\n",
      "Epoch:  1\n",
      "Epoch:  1 Loss:  4.172161213652102 Error:  4.306111209272982 %\n",
      "Epoch:  2\n",
      "Epoch:  2 Loss:  6.153905257493436 Error:  4.094369442921556 %\n",
      "Epoch:  3\n",
      "Epoch:  3 Loss:  7.562403797389504 Error:  3.927682239763037 %\n",
      "Epoch:  4\n",
      "Epoch:  4 Loss:  8.713998097859458 Error:  3.8084272284083025 %\n",
      "Epoch:  5\n",
      "Epoch:  5 Loss:  9.626381077452335 Error:  3.7071011447442506 %\n",
      "Epoch:  6\n",
      "Epoch:  6 Loss:  10.304158961701535 Error:  3.631120140547167 %\n",
      "Epoch:  7\n",
      "Epoch:  7 Loss:  10.755046159207463 Error:  3.576825105189206 %\n",
      "Epoch:  8\n",
      "Epoch:  8 Loss:  11.044237562282357 Error:  3.5255348901013415 %\n",
      "Epoch:  9\n",
      "Epoch:  9 Loss:  11.179905003416325 Error:  3.478171366328251 %\n",
      "Epoch:  10\n",
      "Epoch:  10 Loss:  11.114354761774669 Error:  3.433012187168627 %\n",
      "Epoch:  11\n",
      "Epoch:  11 Loss:  10.843725288699487 Error:  3.393263398797926 %\n",
      "Epoch:  12\n",
      "Epoch:  12 Loss:  10.585745714381783 Error:  3.3614709460271333 %\n",
      "Epoch:  13\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhr_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, subjects_adj, subjects_labels, args)\u001b[39m\n\u001b[32m     26\u001b[39m eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO=\u001b[33m'\u001b[39m\u001b[33mU\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     28\u001b[39m U_hr = U_hr.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m model_outputs, net_outs, start_gcn_outs, layer_outs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m mse_loss = args[\u001b[33m\"\u001b[39m\u001b[33mlmbda\u001b[39m\u001b[33m\"\u001b[39m] * criterion(net_outs, start_gcn_outs) + criterion(\n\u001b[32m     33\u001b[39m \tmodel.layer.weights, U_hr) + criterion(model_outputs, padded_hr)\n\u001b[32m     35\u001b[39m error = criterion(model_outputs, padded_hr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DGL-Group-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DGL-Group-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mAGSRNet.forward\u001b[39m\u001b[34m(self, lr)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m.net_outs, \u001b[38;5;28mself\u001b[39m.start_gcn_outs = \u001b[38;5;28mself\u001b[39m.net(A, I)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mself\u001b[39m.outputs, \u001b[38;5;28mself\u001b[39m.Z = \u001b[38;5;28mself\u001b[39m.layer(A, \u001b[38;5;28mself\u001b[39m.net_outs)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28mself\u001b[39m.hidden1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgc1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mself\u001b[39m.hidden2 = \u001b[38;5;28mself\u001b[39m.gc2(\u001b[38;5;28mself\u001b[39m.hidden1, \u001b[38;5;28mself\u001b[39m.outputs)\n\u001b[32m     27\u001b[39m z = \u001b[38;5;28mself\u001b[39m.hidden2\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DGL-Group-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DGL-Group-Project/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mGraphConvolution.forward\u001b[39m\u001b[34m(self, input, adj)\u001b[39m\n\u001b[32m     57\u001b[39m support = torch.mm(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.weight)\n\u001b[32m     58\u001b[39m output = torch.mm(adj, support)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DGL-Group-Project/venv/lib/python3.12/site-packages/torch/nn/functional.py:1704\u001b[39m, in \u001b[36mrelu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   1702\u001b[39m     result = torch.relu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m   1703\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1704\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/DGL-Group-Project/venv/lib/python3.12/site-packages/torch/fx/traceback.py:175\u001b[39m, in \u001b[36mformat_stack\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta.get(\u001b[33m\"\u001b[39m\u001b[33mstack_trace\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    174\u001b[39m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtraceback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/traceback.py:41\u001b[39m, in \u001b[36mformat_list\u001b[39m\u001b[34m(extracted_list)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mformat_list\u001b[39m(extracted_list):\n\u001b[32m     30\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Format a list of tuples or FrameSummary objects for printing.\u001b[39;00m\n\u001b[32m     31\u001b[39m \n\u001b[32m     32\u001b[39m \u001b[33;03m    Given a list of tuples or FrameSummary objects as returned by\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m \u001b[33;03m    whose source text line is not None.\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStackSummary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_list\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/traceback.py:543\u001b[39m, in \u001b[36mStackSummary.format\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    541\u001b[39m count = \u001b[32m0\u001b[39m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m frame_summary \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m     formatted_frame = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_frame_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_summary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    544\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m formatted_frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    545\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/traceback.py:475\u001b[39m, in \u001b[36mStackSummary.format_frame_summary\u001b[39m\u001b[34m(self, frame_summary)\u001b[39m\n\u001b[32m    473\u001b[39m line = frame_summary._original_line\n\u001b[32m    474\u001b[39m orig_line_len = \u001b[38;5;28mlen\u001b[39m(line)\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m frame_line_len = \u001b[38;5;28mlen\u001b[39m(\u001b[43mframe_summary\u001b[49m\u001b[43m.\u001b[49m\u001b[43mline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    476\u001b[39m stripped_characters = orig_line_len - frame_line_len\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    478\u001b[39m     frame_summary.colno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    479\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m frame_summary.end_colno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    480\u001b[39m ):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train(model, lr_train, hr_train, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
