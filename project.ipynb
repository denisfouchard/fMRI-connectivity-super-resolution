{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from metrics import evaluation_metrics\n",
    "\n",
    "from slim import SLIMDataModule\n",
    "import torch.nn as nn\n",
    "\n",
    "# Instantiate the DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lol\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_module = SLIMDataModule(data_dir=\"./data\", batch_size=1)\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "# Get first batch\n",
    "batch = next(iter(train_dataloader))\n",
    "# Visualise the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([112, 112])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_normalize(A_tilde):\n",
    "    \"\"\"\n",
    "    Performs symmetric normalization of A_tilde (Adj. matrix with self loops):\n",
    "      A_norm = D^{-1/2} * A_tilde * D^{-1/2}\n",
    "    Where D_{ii} = sum of row i in A_tilde.\n",
    "\n",
    "    A_tilde (N, N): Adj. matrix with self loops\n",
    "    Returns:\n",
    "      A_norm : (N, N)\n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-5\n",
    "    d = A_tilde.sum(dim=1) + eps\n",
    "    D_inv = torch.diag(torch.pow(d, -0.5))\n",
    "    return D_inv @ A_tilde @ D_inv\n",
    "\n",
    "\n",
    "def batch_normalize(batch):\n",
    "    batch_n = torch.zeros_like(batch)\n",
    "    for i, A in enumerate(batch):\n",
    "        batch_n[i] = symmetric_normalize(A + torch.eye(n=A.shape[0]))\n",
    "    return batch_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    num_epochs=100,\n",
    "    lr=0.01,\n",
    "    validate_every=1,\n",
    "    patience=10,\n",
    "    criterion=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model, validate every 'validate_every' epochs, and pick the\n",
    "    checkpoint with best validation accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : torch.nn.Module\n",
    "        The PyTorch model to train.\n",
    "    train_dataloader : torch.utils.data.DataLoader\n",
    "        DataLoader for the training set.\n",
    "    val_dataloader : torch.utils.data.DataLoader\n",
    "        DataLoader for the validation set.\n",
    "    num_epochs : int\n",
    "        Number of training epochs.\n",
    "    lr : float\n",
    "        Learning rate for the optimizer.\n",
    "    validate_every : int\n",
    "        Validate (and possibly checkpoint) every 'validate_every' epochs.\n",
    "    patience : int\n",
    "        Patience for learning rate scheduler.\n",
    "    criterion : torch.nn.Module\n",
    "        Loss function.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    best_loss_history : list\n",
    "        The training loss history across epochs.\n",
    "    best_model_state_dict : dict\n",
    "        The state dictionary of the model achieving the best validation accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=patience\n",
    "    )\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_val_loss = torch.inf\n",
    "    best_model_state_dict = None\n",
    "    val_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(range(num_epochs))\n",
    "    for epoch in progress_bar:\n",
    "        progress_bar.set_description(f\"Epoch {epoch}|{num_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            inputs, targets = batch\n",
    "            # inputs = batch_normalize(inputs)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass on training data\n",
    "            outputs = model.forward(inputs, **kwargs)\n",
    "            loss = criterion(outputs, targets.to(model.device))\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record training loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_dataloader)\n",
    "        train_loss_history.append(avg_loss)\n",
    "\n",
    "        # Validation step\n",
    "        if (epoch + 1) % validate_every == 0 or (epoch + 1) == num_epochs:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            for batch in val_dataloader:\n",
    "                inputs, targets = batch\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                val_loss += criterion(outputs, targets.to(model.device)).item()\n",
    "\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_loss_history.append(val_loss)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            lr = get_lr(optimizer)\n",
    "\n",
    "            # Check if this is the best f1 score so far\n",
    "            if val_loss > best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            if lr < 1e-5:\n",
    "                break\n",
    "\n",
    "        progress_bar.set_postfix({\"train_loss\": avg_loss, \"val_loss\": val_loss})\n",
    "\n",
    "    # If we have a best model, load it\n",
    "    if best_model_state_dict is not None:\n",
    "        model.load_state_dict(best_model_state_dict)\n",
    "\n",
    "    return train_loss_history, val_loss_history, best_model_state_dict\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Runs forward pass, calculates binary predictions (threshold=0.5),\n",
    "    and returns the accuracy score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    eval_metrics = {\n",
    "        \"mae\": 0,\n",
    "        \"pcc\": 0,\n",
    "        \"js_dis\": 0,\n",
    "        \"avg_mae_bc\": 0,\n",
    "        \"avg_mae_ec\": 0,\n",
    "        \"avg_mae_pc\": 0,\n",
    "    }\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs, targets = batch\n",
    "        inputs.to(model.device)\n",
    "        outputs.to(model.device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        val_loss += criterion(targets, outputs).item()\n",
    "        batch_metrics = evaluation_metrics(\n",
    "            outputs.detach().numpy(), targets.detach().numpy()\n",
    "        )\n",
    "\n",
    "        for k, v in batch_metrics.items():\n",
    "            eval_metrics[k] += v\n",
    "\n",
    "    val_loss /= len(dataloader)\n",
    "    for v in eval_metrics.values():\n",
    "        v /= len(dataloader)\n",
    "    return val_loss, eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 15, 189])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(32, 15, 112)\n",
    "\n",
    "new_x = torch.nn.functional.interpolate(X, size=(189), mode=\"linear\").squeeze(0)\n",
    "new_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting it all together - code taken from https://github.com/HongyangGao/Graph-U-Nets/tree/master\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def reconstruct_adjacency(X, threshold=0.15):\n",
    "    \"\"\"\n",
    "    Reconstruct adjacency from node embeddings while preserving fMRI-like structure.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Node embeddings of shape [num_nodes, hidden_dim]\n",
    "        threshold (float): Value below which edges are removed for sparsity\n",
    "\n",
    "    Returns:\n",
    "        adj (torch.Tensor): Reconstructed weighted adjacency matrix\n",
    "    \"\"\"\n",
    "    # Normalize embeddings to unit length (cosine similarity instead of raw dot product)\n",
    "    X_norm = F.normalize(X, p=2, dim=1)  # [num_nodes, hidden_dim]\n",
    "    # Compute cosine similarity matrix\n",
    "    adj = X_norm @ X_norm.T  # Values in range [-1, 1]\n",
    "\n",
    "    # adj = torch.sigmoid(adj)\n",
    "\n",
    "    # Apply sparsification: Keep only values above threshold\n",
    "    adj = torch.where(adj > threshold, adj, torch.zeros_like(adj))\n",
    "\n",
    "    return adj\n",
    "\n",
    "\n",
    "class GraphUpsampler(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        hidden_dim,\n",
    "        n_nodes,\n",
    "        m_nodes,\n",
    "        act,\n",
    "        drop_p,\n",
    "        num_iterations=3,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - in_dim: Input node feature dimension\n",
    "        - hidden_dim: Hidden dimension for message passing\n",
    "        - num_iterations: Number of iterative updates\n",
    "        - upsample_factor: Factor by which to increase node count\n",
    "        \"\"\"\n",
    "        super(GraphUpsampler, self).__init__()\n",
    "        self.num_iterations = num_iterations\n",
    "        self.n_nodes = n_nodes\n",
    "        self.m_nodes = m_nodes\n",
    "\n",
    "        # Message passing layers\n",
    "        self.conv1 = GCN(in_dim, hidden_dim, act, drop_p)\n",
    "        self.conv2 = GCN(hidden_dim, hidden_dim, act, drop_p)\n",
    "\n",
    "        # MLP for new node generation\n",
    "        self.upsample_mlp = nn.Linear(n_nodes, m_nodes - n_nodes)\n",
    "\n",
    "    def forward(self, X, A):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - x: Node features [num_nodes, in_dim]\n",
    "        - adj_matrix: Initial adjacency matrix [num_nodes, num_nodes]\n",
    "\n",
    "        Returns:\n",
    "        - Upsampled adjacency matrix [self.m_nodes, self.m_nodes]\n",
    "        - Upsampled node features [new_num_nodes, in_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        # Generate new nodes by transforming existing ones\n",
    "        new_nodes = torch.sigmoid(self.upsample_mlp(X.T).T)  # [num_nodes, in_dim]\n",
    "        # Concatenate old and new nodes\n",
    "        X_upsampled = torch.cat([X, new_nodes], dim=0)\n",
    "\n",
    "        A_upsampled = reconstruct_adjacency(X=X_upsampled)\n",
    "\n",
    "        # print(\"Mean : \", A_upsampled.mean().item(), \"Std :\", A_upsampled.std().item())\n",
    "\n",
    "        # Message passing to refine embeddings\n",
    "        for _ in range(self.num_iterations):\n",
    "            X_upsampled = self.conv1(A_upsampled, X_upsampled)\n",
    "            X_upsampled = F.relu(X_upsampled)\n",
    "            A_upsampled = reconstruct_adjacency(X_upsampled)\n",
    "\n",
    "            X_upsampled = self.conv2(A_upsampled, X_upsampled)\n",
    "            X_upsampled = F.relu(X_upsampled)\n",
    "\n",
    "            # Reconstruct adjacency with updated embeddings\n",
    "            A_upsampled = reconstruct_adjacency(A_upsampled)\n",
    "\n",
    "        return A_upsampled\n",
    "\n",
    "\n",
    "class GraphUnet(nn.Module):\n",
    "\n",
    "    def __init__(self, ks, n_nodes, m_nodes, dim, act, drop_p):\n",
    "        super(GraphUnet, self).__init__()\n",
    "        self.ks = ks\n",
    "        self.dim = dim\n",
    "        self.bottom_gcn = GCN(dim, dim, act, drop_p)\n",
    "        self.down_gcns = nn.ModuleList()\n",
    "        self.up_gcns = nn.ModuleList()\n",
    "        self.pools = nn.ModuleList()\n",
    "        self.unpools = nn.ModuleList()\n",
    "        self.upsampler = GraphUpsampler(\n",
    "            in_dim=dim,\n",
    "            hidden_dim=dim,\n",
    "            n_nodes=n_nodes,\n",
    "            m_nodes=m_nodes,\n",
    "            act=act,\n",
    "            drop_p=drop_p,\n",
    "        )\n",
    "        self.l_n = len(ks)\n",
    "        for i in range(self.l_n):\n",
    "            self.down_gcns.append(GCN(dim, dim, act, drop_p))\n",
    "            self.up_gcns.append(GCN(dim, dim, act, drop_p))\n",
    "            self.pools.append(Pool(ks[i], dim, drop_p))\n",
    "            self.unpools.append(Unpool(dim, dim, drop_p))\n",
    "\n",
    "        self.node_features = nn.Parameter(torch.randn(n_nodes, dim))\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, A: torch.Tensor):\n",
    "        # Process A\n",
    "        A = A.squeeze(0)\n",
    "        A = torch.where(A > 0.2, A, torch.zeros_like(A))\n",
    "        A = A + torch.eye(A.shape[0])\n",
    "        A = symmetric_normalize(A)\n",
    "        A = A.to(self.device)\n",
    "\n",
    "        X = self.node_features\n",
    "        adj_ms = []\n",
    "        indices_list = []\n",
    "        down_outs = []\n",
    "        org_A = torch.tensor(A)\n",
    "        org_X = torch.tensor(X)\n",
    "        for i in range(self.l_n):\n",
    "            X = self.down_gcns[i](A, X)\n",
    "            adj_ms.append(A)\n",
    "            down_outs.append(X)\n",
    "            A, X, idx = self.pools[i](A, X)\n",
    "            indices_list.append(idx)\n",
    "\n",
    "        X = self.bottom_gcn(A, X)\n",
    "        for i in range(self.l_n):\n",
    "            up_idx = self.l_n - i - 1\n",
    "            A, idx = adj_ms[up_idx], indices_list[up_idx]\n",
    "            A, X = self.unpools[i](A, X, down_outs[up_idx], idx)\n",
    "            X = self.up_gcns[i](A, X)\n",
    "            # X = X.add(down_outs[up_idx])\n",
    "\n",
    "        # X = X.add(org_X)\n",
    "\n",
    "        A_upsampled = self.upsampler.forward(X, org_A)\n",
    "        return A_upsampled\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, act, p):\n",
    "        super(GCN, self).__init__()\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "        self.act = act\n",
    "        self.drop = nn.Dropout(p=p) if p > 0.0 else nn.Identity()\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        h = self.drop(h)  # they have added dropout\n",
    "        h = torch.matmul(g, h)\n",
    "        h = self.proj(h)\n",
    "        h = self.act(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class Pool(nn.Module):\n",
    "\n",
    "    def __init__(self, k, in_dim, p):\n",
    "        super(Pool, self).__init__()\n",
    "        self.k = k\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.proj = nn.Linear(in_dim, 1)\n",
    "        self.drop = nn.Dropout(p=p) if p > 0 else nn.Identity()  # added dropout here\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        Z = self.drop(h)\n",
    "        weights = self.proj(Z).squeeze()\n",
    "        scores = self.sigmoid(weights)\n",
    "        return top_k_graph(scores, g, h, self.k)\n",
    "\n",
    "\n",
    "class Unpool(nn.Module):\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super(Unpool, self).__init__()\n",
    "\n",
    "    def forward(self, A, X, pre_h, idx):\n",
    "        new_h = X.new_zeros([A.shape[0], X.shape[1]])\n",
    "        new_h[idx] = X\n",
    "        return A, new_h\n",
    "\n",
    "\n",
    "def top_k_graph(scores, A, X, k):\n",
    "    num_nodes = A.shape[0]\n",
    "    values, idx = torch.topk(\n",
    "        scores, max(2, int(k * num_nodes))\n",
    "    )  # make sure k works based on number of current nodes\n",
    "    X_pooled = X[idx, :]\n",
    "    values = torch.unsqueeze(values, -1)\n",
    "    X_pooled = torch.mul(X_pooled, values)\n",
    "    A_pooled = A.bool().float()\n",
    "    A_pooled = (\n",
    "        torch.matmul(A_pooled, A_pooled).bool().float()\n",
    "    )  # second power to reduce chance of isolated nodes\n",
    "    A_pooled = A_pooled[idx, :]\n",
    "    A_pooled = A_pooled[:, idx]\n",
    "    A_pooled = symmetric_normalize(A_pooled)\n",
    "    return A_pooled, X_pooled, idx\n",
    "\n",
    "\n",
    "def symmetric_normalize(A_tilde):\n",
    "    \"\"\"\n",
    "    Performs symmetric normalization of A_tilde (Adj. matrix with self loops):\n",
    "      A_norm = D^{-1/2} * A_tilde * D^{-1/2}\n",
    "    Where D_{ii} = sum of row i in A_tilde.\n",
    "\n",
    "    A_tilde (N, N): Adj. matrix with self loops\n",
    "    Returns:\n",
    "      A_norm : (N, N)\n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-5\n",
    "    d = A_tilde.sum(dim=1) + eps\n",
    "    D_inv = torch.diag(torch.pow(d, -0.5))\n",
    "    return D_inv @ A_tilde @ D_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss function, and optimizer\n",
    "in_dim = batch[0].shape[1]\n",
    "out_dim = batch[1].shape[1]\n",
    "dim = 100\n",
    "n_layers = 5\n",
    "model = GraphUnet(\n",
    "    ks=[0.5, 0.5, 0.5],\n",
    "    n_nodes=in_dim,\n",
    "    m_nodes=out_dim,\n",
    "    dim=15,\n",
    "    act=torch.sigmoid,\n",
    "    drop_p=0.2,\n",
    ")\n",
    "model.to(torch.device(\"mps\"))\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0|100:   0%|          | 0/100 [00:00<?, ?it/s]/var/folders/0k/b57mm7p92n705vxsd5p1t6gc0000gn/T/ipykernel_46142/1684190514.py:145: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  org_A = torch.tensor(A)\n",
      "/var/folders/0k/b57mm7p92n705vxsd5p1t6gc0000gn/T/ipykernel_46142/1684190514.py:146: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  org_X = torch.tensor(X)\n",
      "/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([1, 189, 189])) that is different to the input size (torch.Size([189, 189])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 2|100:   2%|▏         | 2/100 [00:10<08:50,  5.42s/it, train_loss=0.588, val_loss=0.597]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[339], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTORCH_ENABLE_MPS_FALLBACK\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m train_losses, val_losses, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[283], line 72\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, num_epochs, lr, validate_every, patience, criterion, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Forward pass on training data\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m     74\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[337], line 159\u001b[0m, in \u001b[0;36mGraphUnet.forward\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    157\u001b[0m     A, idx \u001b[38;5;241m=\u001b[39m adj_ms[up_idx], indices_list[up_idx]\n\u001b[1;32m    158\u001b[0m     A, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munpools[i](A, X, down_outs[up_idx], idx)\n\u001b[0;32m--> 159\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_gcns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# X = X.add(down_outs[up_idx])\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# X = X.add(org_X)\u001b[39;00m\n\u001b[1;32m    164\u001b[0m A_upsampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsampler\u001b[38;5;241m.\u001b[39mforward(X, org_A)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[337], line 177\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, g, h)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, g, h):\n\u001b[0;32m--> 177\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# they have added dropout\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(g, h)\n\u001b[1;32m    179\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(h)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/modules/dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/site-packages/torch/nn/functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1426\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "train_losses, val_losses, _ = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=data_module.train_dataloader(),\n",
    "    val_dataloader=data_module.val_dataloader(),\n",
    "    num_epochs=100,\n",
    "    lr=0.01,\n",
    "    validate_every=1,\n",
    "    patience=50,\n",
    "    criterion=criterion,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_losses, val_losses, title=\"Losses\"):\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_losses=train_losses, val_losses=val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "_, eval_metrics = evaluate_model(\n",
    "    model, data_module.val_dataloader(), criterion=criterion\n",
    ")\n",
    "\n",
    "print(eval_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
