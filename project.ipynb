{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from metrics import evaluation_metrics\n",
    "\n",
    "from slim import SLIMDataModule\n",
    "import torch.nn as nn\n",
    "\n",
    "# Instantiate the DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_module = SLIMDataModule(data_dir=\"./data\")\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "# Get first batch\n",
    "batch = next(iter(train_dataloader))\n",
    "# Visualise the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.6371, 0.3994,  ..., 0.0000, 0.0147, 0.3531],\n",
       "        [0.6371, 0.0000, 0.5750,  ..., 0.0000, 0.0818, 0.2694],\n",
       "        [0.3994, 0.5750, 0.0000,  ..., 0.0000, 0.0000, 0.1344],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5492, 0.0000],\n",
       "        [0.0147, 0.0818, 0.0000,  ..., 0.5492, 0.0000, 0.1841],\n",
       "        [0.3531, 0.2694, 0.1344,  ..., 0.0000, 0.1841, 0.0000]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_normalize(A_tilde):\n",
    "    \"\"\"\n",
    "    Performs symmetric normalization of A_tilde (Adj. matrix with self loops):\n",
    "      A_norm = D^{-1/2} * A_tilde * D^{-1/2}\n",
    "    Where D_{ii} = sum of row i in A_tilde.\n",
    "\n",
    "    A_tilde (N, N): Adj. matrix with self loops\n",
    "    Returns:\n",
    "      A_norm : (N, N)\n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-5\n",
    "    d = A_tilde.sum(dim=1) + eps\n",
    "    D_inv = torch.diag(torch.pow(d, -0.5))\n",
    "    return D_inv @ A_tilde @ D_inv\n",
    "\n",
    "\n",
    "def batch_normalize(batch):\n",
    "    batch_n = torch.zeros_like(batch)\n",
    "    for i, A in enumerate(batch):\n",
    "        batch_n[i] = symmetric_normalize(A + torch.eye(n=A.shape[0]))\n",
    "    return batch_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    num_epochs=100,\n",
    "    lr=0.01,\n",
    "    validate_every=1,\n",
    "    patience=10,\n",
    "    criterion=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model, validate every 'validate_every' epochs, and pick the\n",
    "    checkpoint with best validation accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : torch.nn.Module\n",
    "        The PyTorch model to train.\n",
    "    train_dataloader : torch.utils.data.DataLoader\n",
    "        DataLoader for the training set.\n",
    "    val_dataloader : torch.utils.data.DataLoader\n",
    "        DataLoader for the validation set.\n",
    "    num_epochs : int\n",
    "        Number of training epochs.\n",
    "    lr : float\n",
    "        Learning rate for the optimizer.\n",
    "    validate_every : int\n",
    "        Validate (and possibly checkpoint) every 'validate_every' epochs.\n",
    "    patience : int\n",
    "        Patience for learning rate scheduler.\n",
    "    criterion : torch.nn.Module\n",
    "        Loss function.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    best_loss_history : list\n",
    "        The training loss history across epochs.\n",
    "    best_model_state_dict : dict\n",
    "        The state dictionary of the model achieving the best validation accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=patience\n",
    "    )\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_val_loss = torch.inf\n",
    "    best_model_state_dict = None\n",
    "    val_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(range(num_epochs))\n",
    "    for epoch in progress_bar:\n",
    "        progress_bar.set_description(f\"Epoch {epoch}|{num_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            inputs, targets = batch\n",
    "            inputs = batch_normalize(inputs)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass on training data\n",
    "            outputs = model.forward(inputs, **kwargs)\n",
    "            loss = criterion(outputs, targets.to(model.device))\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record training loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_dataloader)\n",
    "        train_loss_history.append(avg_loss)\n",
    "\n",
    "        # Validation step\n",
    "        if (epoch + 1) % validate_every == 0 or (epoch + 1) == num_epochs:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            for batch in val_dataloader:\n",
    "                inputs, targets = batch\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                val_loss += criterion(outputs, targets.to(model.device)).item()\n",
    "\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_loss_history.append(val_loss)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            lr = get_lr(optimizer)\n",
    "\n",
    "            # Check if this is the best f1 score so far\n",
    "            if val_loss > best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            if lr < 1e-5:\n",
    "                break\n",
    "\n",
    "        progress_bar.set_postfix({\"train_loss\": avg_loss, \"val_loss\": val_loss})\n",
    "\n",
    "    # If we have a best model, load it\n",
    "    if best_model_state_dict is not None:\n",
    "        model.load_state_dict(best_model_state_dict)\n",
    "\n",
    "    return train_loss_history, val_loss_history, best_model_state_dict\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Runs forward pass, calculates binary predictions (threshold=0.5),\n",
    "    and returns the accuracy score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    eval_metrics = {\n",
    "        \"mae\": 0,\n",
    "        \"pcc\": 0,\n",
    "        \"js_dis\": 0,\n",
    "        \"avg_mae_bc\": 0,\n",
    "        \"avg_mae_ec\": 0,\n",
    "        \"avg_mae_pc\": 0,\n",
    "    }\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs, targets = batch\n",
    "        inputs.to(model.device)\n",
    "        outputs.to(model.device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        val_loss += criterion(targets, outputs).item()\n",
    "        batch_metrics = evaluation_metrics(\n",
    "            outputs.detach().numpy(), targets.detach().numpy()\n",
    "        )\n",
    "\n",
    "        for k, v in batch_metrics.items():\n",
    "            eval_metrics[k] += v\n",
    "\n",
    "    val_loss /= len(dataloader)\n",
    "    for v in eval_metrics.values():\n",
    "        v /= len(dataloader)\n",
    "    return val_loss, eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 15, 189])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn(32, 15, 112)\n",
    "\n",
    "new_x = torch.nn.functional.interpolate(X, size=(189), mode=\"linear\").squeeze(0)\n",
    "new_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.conv import SAGEConv, GCNConv\n",
    "import torch.nn as nn\n",
    "\n",
    "num_nodes = 1000  # Adjust based on your dataset\n",
    "embedding_dim = 128  # Dimension of node embeddings\n",
    "\n",
    "# Trainable node embeddings\n",
    "node_embeddings = nn.Embedding(num_nodes, embedding_dim)\n",
    "\n",
    "\n",
    "class GCNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer of a Graph Convolutional Network (GCN).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, use_nonlinearity=True):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.use_nonlinearity = use_nonlinearity\n",
    "        self.Omega = nn.Parameter(\n",
    "            torch.randn(input_dim, output_dim)\n",
    "            * torch.sqrt(torch.tensor(2.0) / (input_dim + output_dim))\n",
    "        )\n",
    "        self.beta = nn.Parameter(torch.zeros(output_dim))\n",
    "\n",
    "    def forward(self, A_normalized, H_k):\n",
    "        agg = torch.matmul(A_normalized, H_k)  # local agg\n",
    "        H_k_next = torch.matmul(agg, self.Omega) + self.beta\n",
    "        return nn.functional.relu(H_k_next) if self.use_nonlinearity else H_k_next\n",
    "\n",
    "\n",
    "# GraphSAGE model\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, hidden_channels, out_size, n_layers: int = 2):\n",
    "        super().__init__()\n",
    "        self.in_channels = hidden_channels\n",
    "        self.out_size = out_size\n",
    "        self.conv = nn.ModuleList()\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.conv.append(\n",
    "                GCNLayer(input_dim=hidden_channels, output_dim=hidden_channels)\n",
    "            )\n",
    "        self.conv.append(\n",
    "            GCNLayer(\n",
    "                input_dim=hidden_channels,\n",
    "                output_dim=hidden_channels,\n",
    "                use_nonlinearity=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, A):\n",
    "        A = A.to(self.device)\n",
    "        X = torch.ones(\n",
    "            A.shape[0],\n",
    "            A.shape[1],\n",
    "            self.in_channels,\n",
    "            dtype=torch.float32,\n",
    "            device=self.device,\n",
    "        )\n",
    "        for layer in self.conv:\n",
    "            X = layer(A, X)\n",
    "        X = X.permute(0, 2, 1)\n",
    "        X = torch.nn.functional.interpolate(\n",
    "            X, size=(self.out_size,), mode=\"linear\"\n",
    "        ).squeeze(0)\n",
    "        X = X.permute(0, 2, 1)\n",
    "        A_pred = torch.zeros(\n",
    "            A.shape[0],\n",
    "            self.out_size,\n",
    "            self.out_size,\n",
    "            dtype=torch.float32,\n",
    "            device=self.device,\n",
    "        )\n",
    "        for i, x in enumerate(X):\n",
    "            A_pred[i] = torch.sigmoid(x @ x.T)\n",
    "        A_pred = A_pred * (\n",
    "            A_pred > 0.2\n",
    "        )  # Thresholding to preserve sparse brain connectivity\n",
    "        return A_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss function, and optimizer\n",
    "in_dim = batch[0].shape[1]\n",
    "out_dim = batch[1].shape[1]\n",
    "dim = 100\n",
    "n_layers = 5\n",
    "\n",
    "model = GraphSAGE(out_size=out_dim, hidden_channels=dim, n_layers=n_layers)\n",
    "model.to(torch.device(\"mps\"))\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99|100: 100%|██████████| 100/100 [00:13<00:00,  7.47it/s, train_loss=0.104, val_loss=0.549]\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses, _ = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=data_module.train_dataloader(),\n",
    "    val_dataloader=data_module.val_dataloader(),\n",
    "    num_epochs=100,\n",
    "    lr=0.01,\n",
    "    validate_every=1,\n",
    "    patience=50,\n",
    "    criterion=criterion,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_losses, val_losses, title=\"Losses\"):\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_losses=train_losses, val_losses=val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "_, eval_metrics = evaluate_model(\n",
    "    model, data_module.val_dataloader(), criterion=criterion\n",
    ")\n",
    "\n",
    "print(eval_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
