{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from metrics import evaluation_metrics\n",
    "\n",
    "from slim import SLIMDataModule\n",
    "\n",
    "# Instantiate the DataModule\n",
    "\n",
    "data_module = SLIMDataModule(data_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_dataloader = data_module.train_dataloader()\n",
    "# Get first batch\n",
    "batch = next(iter(train_dataloader))\n",
    "# Visualise the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    num_epochs=100,\n",
    "    lr=0.01,\n",
    "    validate_every=1,\n",
    "    patience=10,\n",
    "    criterion=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model, validate every 'validate_every' epochs, and pick the\n",
    "    checkpoint with best validation accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : torch.nn.Module\n",
    "        The PyTorch model to train.\n",
    "    train_dataloader : torch.utils.data.DataLoader\n",
    "        DataLoader for the training set.\n",
    "    val_dataloader : torch.utils.data.DataLoader\n",
    "        DataLoader for the validation set.\n",
    "    num_epochs : int\n",
    "        Number of training epochs.\n",
    "    lr : float\n",
    "        Learning rate for the optimizer.\n",
    "    validate_every : int\n",
    "        Validate (and possibly checkpoint) every 'validate_every' epochs.\n",
    "    patience : int\n",
    "        Patience for learning rate scheduler.\n",
    "    criterion : torch.nn.Module\n",
    "        Loss function.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    best_loss_history : list\n",
    "        The training loss history across epochs.\n",
    "    best_model_state_dict : dict\n",
    "        The state dictionary of the model achieving the best validation accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=patience\n",
    "    )\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    best_val_loss = torch.inf\n",
    "    best_model_state_dict = None\n",
    "    val_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(range(num_epochs))\n",
    "    for epoch in progress_bar:\n",
    "        progress_bar.set_description(f\"Epoch {epoch}|{num_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            inputs, targets = batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass on training data\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.to(model.device))\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record training loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_dataloader)\n",
    "        train_loss_history.append(avg_loss)\n",
    "\n",
    "        # Validation step\n",
    "        if (epoch + 1) % validate_every == 0 or (epoch + 1) == num_epochs:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            for batch in val_dataloader:\n",
    "                inputs, targets = batch\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                val_loss += criterion(outputs, targets.to(model.device)).item()\n",
    "\n",
    "            val_loss /= len(val_dataloader)\n",
    "            val_loss_history.append(val_loss)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            lr = get_lr(optimizer)\n",
    "\n",
    "            # Check if this is the best f1 score so far\n",
    "            if val_loss > best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            if lr < 1e-5:\n",
    "                break\n",
    "\n",
    "        progress_bar.set_postfix({\"train_loss\": avg_loss, \"val_loss\": val_loss})\n",
    "\n",
    "    # If we have a best model, load it\n",
    "    if best_model_state_dict is not None:\n",
    "        model.load_state_dict(best_model_state_dict)\n",
    "\n",
    "    return train_loss_history, val_loss_history, best_model_state_dict\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Runs forward pass, calculates binary predictions (threshold=0.5),\n",
    "    and returns the accuracy score.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    eval_metrics = {\n",
    "        \"mae\": 0,\n",
    "        \"pcc\": 0,\n",
    "        \"js_dis\": 0,\n",
    "        \"avg_mae_bc\": 0,\n",
    "        \"avg_mae_ec\": 0,\n",
    "        \"avg_mae_pc\": 0,\n",
    "    }\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs, targets = batch\n",
    "        inputs.to(model.device)\n",
    "        outputs.to(model.device)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        val_loss += criterion(targets, outputs).item()\n",
    "        batch_metrics = evaluation_metrics(\n",
    "            outputs.detach().numpy(), targets.detach().numpy()\n",
    "        )\n",
    "\n",
    "        for k, v in batch_metrics.items():\n",
    "            eval_metrics[k] += v\n",
    "\n",
    "    val_loss /= len(dataloader)\n",
    "    for v in eval_metrics.values():\n",
    "        v /= len(dataloader)\n",
    "    return val_loss, eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive MLP model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SuperResMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super().__init__()\n",
    "        h = int(np.sqrt(output_size))\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(in_features=input_size, out_features=hidden_dim),\n",
    "                nn.BatchNorm1d(num_features=hidden_dim),\n",
    "                nn.Dropout(p=0.1),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "        )\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.layers.append(\n",
    "                nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "            )\n",
    "            self.layers.append(nn.BatchNorm1d(num_features=hidden_dim))\n",
    "            self.layers.append(nn.Dropout(p=0.1))\n",
    "            self.layers.append(nn.ReLU())\n",
    "\n",
    "        self.layers.append(nn.Linear(in_features=hidden_dim, out_features=output_size))\n",
    "        self.layers.append(nn.Unflatten(dim=1, unflattened_size=(h, h)))\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, samples: torch.Tensor):\n",
    "        # Flatten the input if it's a 2D matrix\n",
    "        x = samples.to(self.device)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model, loss function, and optimizer\n",
    "input_size = batch[0].shape[1] ** 2\n",
    "output_size = batch[1].shape[1] ** 2\n",
    "hidden_dim = 1000\n",
    "n_layers = 3\n",
    "\n",
    "model = SuperResMLP(input_size, output_size, hidden_dim, n_layers).to(\n",
    "    torch.device(\"mps\")\n",
    ")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "train_losses, val_losses, _ = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=data_module.train_dataloader(),\n",
    "    val_dataloader=data_module.val_dataloader(),\n",
    "    num_epochs=100,\n",
    "    lr=0.001,\n",
    "    validate_every=1,\n",
    "    patience=5,\n",
    "    criterion=criterion,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_losses, val_losses, title=\"Losses\"):\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_losses=train_losses, val_losses=val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "_, eval_metrics = evaluate_model(\n",
    "    model, data_module.val_dataloader(), criterion=criterion\n",
    ")\n",
    "\n",
    "print(eval_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
